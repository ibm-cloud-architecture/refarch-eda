---
title: Kafka Streams Test Lab 1
description: Using Kafka Streams Test Suite to test Streams
---

<AnchorLinks>
    <AnchorLink>Overview</AnchorLink>
    <AnchorLink>Scenario Prerequisites</AnchorLink>
    <AnchorLink>Setting up the Quarkus Application</AnchorLink>
    <AnchorLink>Creating your first Test Class</AnchorLink>
    <AnchorLink>More Robust Kafka Streams Testing</AnchorLink>
    <AnchorLink>Next Steps</AnchorLink>
</AnchorLinks>


## Overview
- In this lab scenario we're going to use [Quarkus](https://quarkus.io) - a subatomic and supersonic framework for Java for
the purposes of this lab.
- We will be testing using [Apache Kafka Streams](https://kafka.apache.org/documentation/streams/) TestDriver to mimic a Topology, a Stream and Table.
- While using the TestDriver we will perform operations such as groupBy, join with another Stream or Kafka Table.
- Lastly and optionally (in Lab 2) we will use Kafka Streams to send events to a Kafka Topic on IBM Event Streams on Cloud Pak for Integration.



## Scenario Prerequisites
**Java**
- For the purposes of this lab we suggest Java 8+

**Maven**
- Maven will be needed for bootstrapping our application from the command-line and running
our application.

**An IDE of your choice**
- Ideally an IDE that supports Quarkus (such as Visual Studio Code)


## Setting up the Quarkus Application
- We will bootstrap the Quarkus application with the following Maven command

```shell
mvn io.quarkus:quarkus-maven-plugin:1.6.0.Final:create \
    -DprojectGroupId={com.ibm} \
    -DprojectArtifactId={quarkus-kstreams-lab} \
    -Dextensions="kafka,kafka-streams,resteasy-jsonb,quarkus-kafka-streams"
```

You can replace the fields within {} as you like.

- Since we will be using the Kafka Streams testing functionality we will need to edit the `pom.xml` to add
the dependency to our project. Open `pom.xml` and add the following.

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams-test-utils</artifactId>
    <version>2.5.0</version>
    <scope>test</scope>
</dependency>
```

## Creating your first Test Class

- Now let's create our first Test Class.
```src/test/java/.../TestLoadKtableFromTopic.java```
You can customize this file path post `src/test/java` to however you see fit.


- Open the `TestLoadKtableFromTopic.java` file and paste the following content.
```java
package com.ibm.garage.cpat.lab;

import java.util.Properties;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.TestInputTopic;
import org.apache.kafka.streams.TestOutputTopic;
import org.apache.kafka.streams.TopologyTestDriver;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KTable;
import org.apache.kafka.streams.kstream.Materialized;
import org.apache.kafka.streams.processor.StateStore;
import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;
import org.apache.kafka.streams.state.KeyValueIterator;
import org.apache.kafka.streams.state.KeyValueStore;
import org.apache.kafka.streams.state.Stores;
import org.apache.kafka.streams.state.ValueAndTimestamp;
import org.junit.jupiter.api.AfterAll;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;

import io.quarkus.test.junit.QuarkusTest;

/**
 * This is a simple example of loading some reference data from stream into a Ktable for
 * lookup. It uses a persistent state store.
 */
@QuarkusTest
public class TestLoadKtableFromTopic {
    private static TopologyTestDriver testDriver;
    private static String companySectorsTopic = "sector-types";
    private static String storeName = "sector-types-store";

    private static TestInputTopic<String, String> inTopic;
    private static TestOutputTopic<String, Long> outTopic;
    private static TestOutputTopic<String, String> errorTopic;

    public static Properties getStreamsConfig() {
        final Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "kstream-lab1");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummmy:1234");
        return props;
    }

    @BeforeAll
    public static void buildTopology(){
        final StreamsBuilder builder = new StreamsBuilder();
        // Adding a state store is a simple matter of creating a StoreSupplier
        // instance with one of the static factory methods on the Stores class.
        // all persistent StateStore instances provide local storage using RocksDB
        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);

        // A KTable is created from the companySectorsTopic, with key and value deserialized.
        // With Materialized.as() causing the Table to force a state store materialization (storeSupplier).
        KTable<String, String> sectorTypeTable = builder.table(companySectorsTopic,
                Consumed.with(Serdes.String(), Serdes.String()),
                Materialized.as(storeSupplier));

        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());
        inTopic = testDriver.createInputTopic(companySectorsTopic, new StringSerializer(), new StringSerializer());

    }

    @AfterAll
    public static void close(){
        testDriver.close();
    }

    @Test
    public void shouldHaveSixSectorTypes(){
        inTopic.pipeInput("C01","Health Care");
        inTopic.pipeInput("C02","Finance");
        inTopic.pipeInput("C03","Consumer Services");
        inTopic.pipeInput("C04","Transportation");
        inTopic.pipeInput("C05","Capital Goods");
        inTopic.pipeInput("C06","Public Utilities");

        KeyValueStore<String,ValueAndTimestamp<String>> store = testDriver.getTimestampedKeyValueStore(storeName);
        Assertions.assertNotNull(store);

        ValueAndTimestamp<String> sector = store.get("C02");
        Assertions.assertNotNull(sector);
        Assertions.assertEquals("Finance", sector.value());
        Assertions.assertEquals(6, store.approximateNumEntries());


        // demonstrate how to get all the values from the table:
        KeyValueIterator<String, ValueAndTimestamp<String>> sectors = store.all();
        while (sectors.hasNext()) {
            KeyValue<String,ValueAndTimestamp<String>> s = sectors.next();
            System.out.println(s.key + ":" + s.value.value());
        }
        for ( StateStore s: testDriver.getAllStateStores().values()) {
            System.out.println(s.name());
        }
    }
}
```

- What the above code does is it uses TopologyTestDriver to mimic a Topology. A Topology is basically a graph of
stream processors (nodes) and the edges between these nodes are the streams. In the first sectinon we instantiate
our `TopologyTestDriver` named `testDriver`, as well as the topic name and store name.



- Test the application by running the following
*Note* You might need to comment out the Quarkus Kafka Streams dependency in `pom.xml` as that dependency requires some
configration in a properties file to pass. This is configured in the `Producing and Consuming to a Kafka Topic on Event Streams` section
further down in the lab.

```shell
./mvnw clean verify
```

- Depending upon versions of the packages brought in, you may see an initial test failure due to needing to update the `src/main/resources/application.properties` file with the following properties: _(The values are insignifcant for the execution of our tests, but are existence of the property is required by the underlying Quarkus & Kafka Streams integration)_
```properties
quarkus.kafka-streams.application-id=my-kafka-streams
quarkus.kafka-streams.topics=topic1
```

- How this test topology creation flow works:
    - A StreamsBuilder object (builder) from the Kafka Streams DSL API is created.
    - A KeyValueBytesStoreSupplier (storeSupplier) is configured with String variable (storeName).
    - A KTable is created reading from the topic (companySectorsTopic), deserialized and materialized as
    the previously create (storeSupplier).
    - A TopologyTestDriver (testDriver) is built from the provided config properties and the KTable within the builder topology.
    - Lastly test input topic (inTopic) is created from the testDriver topology.
    - When `inTopic.pipeInput("C01","Health Care");` is invoked, it populates the topic, which then populates the KTable
    which ultimately persists in a KeyValue State Store.

- You should see the tests pass. These are three simple tests. The first of which checks that the value fetched from
the Kafka Table is not null,the second makes sure that value retrieved from key `C02` is equal to `Finance` and lastly
we make sure that the our state store (which was piped by ways of the Kafka Topic) indeed has six key-value pairs.



## More Robust Kafka Streams Testing

- Now that we have tested some simple functionality by using the Kafka Streams API let's check out some other
operators that we can use.

- Let's create a new class for our Plain Old Java Object (POJO) named FinancialMessage. You can place this where you want
but for simplicity's sake I will use this path. Make sure you remember the path for when you import this class
`src/main/java/com/ibm/garage/cpat/FinancialMessage/FinancialMessage.java`

Now copy and paste the following content into the newly created file.

```java
public class FinancialMessage {

    public String userId;
    public String stockSymbol;
    public String exchangeId;
    public int quantity;
    public double stockPrice;
    public double totalCost;
    public int institutionId;
    public int countryId;
    public boolean technicalValidation;

    public FinancialMessage() {

    }

    public FinancialMessage(String userId, String stockSymbol, String exchangeId,
                            int quantity, double stockPrice, double totalCost,
                            int institutionId, int countryId, boolean technicalValidation) {

        this.userId = userId;
        this.stockSymbol = stockSymbol;
        this.exchangeId = exchangeId;
        this.quantity = quantity;
        this.stockPrice = stockPrice;
        this.totalCost = totalCost;
        this.institutionId = institutionId;
        this.countryId = countryId;
        this.technicalValidation = technicalValidation;
    }
}
```

**Note** - For brevity and simplicity's sake I did not provide any accessors (getters) or mutators (setters). You can
set those at your own discretion.

- Now that we have our Java class, let's create a new and separate Java Test class separate from the `TestLoadKtableFromTopic.java`
for separation of logic. `src/test/java/.../lab/TestFinancialMessage.java`. Again you may use your own
filepath. Copy the contents below.

```java
package com.ibm.garage.cpat.lab;

import java.util.Properties;

import org.apache.kafka.common.serialization.LongDeserializer;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.TestInputTopic;
import org.apache.kafka.streams.TestOutputTopic;
import org.apache.kafka.streams.TopologyTestDriver;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Materialized;
import org.apache.kafka.streams.kstream.Produced;
import org.apache.kafka.streams.kstream.Windowed;
import org.apache.kafka.streams.kstream.WindowedSerdes;
import org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;
import org.apache.kafka.streams.state.KeyValueStore;
import org.apache.kafka.streams.state.Stores;
import org.apache.kafka.streams.state.ValueAndTimestamp;
import org.junit.jupiter.api.AfterAll;
import org.junit.jupiter.api.Assertions;
import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;

import io.quarkus.kafka.client.serialization.JsonbSerde;
import io.quarkus.kafka.client.serialization.JsonbSerializer;
import io.quarkus.test.junit.QuarkusTest;

import com.ibm.garage.cpat.Domain.*;


@QuarkusTest
public class TestFinancialMessage {

    private static TopologyTestDriver testDriver;
    private static String inTopicName = "transactions";
    private static String outTopicName = "output";
    private static String errorTopicName = "errors";
    private static String storeName = "transactionCount";
    private static TestInputTopic<String, FinancialMessage> inTopic;
    private static TestOutputTopic<String, Long> outTopic;
    private static TestOutputTopic<String, String> errorTopic;

    private static final JsonbSerde<FinancialMessage> financialMessageSerde = new JsonbSerde<>(FinancialMessage.class);

    public static Properties getStreamsConfig() {
        final Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "kstream-lab2");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "dummmy:2345");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        //props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, financialMessageSerde);
        return props;
    }

    @BeforeAll
    public static void buildTopology() {
        final StreamsBuilder builder = new StreamsBuilder();
        KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(storeName);

        KStream<String, FinancialMessage> transactionStream =
            builder.stream(
                inTopicName,
                Consumed.with(Serdes.String(), financialMessageSerde)
            );

        // First verify user id is present, if not route to error
        KStream<String, FinancialMessage>[] branches =
                transactionStream.branch(
                    (key, value) -> value.userId == null,
                    (key, value) -> true
                );

        // Handle error by sending to the errors topic.
        branches[0].map(
                 (key, value) -> { return KeyValue.pair(key, "No customer id provided");}
                 )
                .to(
                    errorTopicName, Produced.with(Serdes.String(), Serdes.String())
                );


        // use groupBy to swap the key, then count by customer id,
        branches[1].groupBy(
                    (key, value) -> value.userId
                )
                .count(
                    Materialized.as(storeSupplier)
                )
                .toStream()
                .to(
                    outTopicName,
                    Produced.with(Serdes.String(), Serdes.Long())
            );

        testDriver = new TopologyTestDriver(builder.build(), getStreamsConfig());
        inTopic = testDriver.createInputTopic(inTopicName, new StringSerializer(), new JsonbSerializer<FinancialMessage>());
        //outTopic = testDriver.createOutputTopic(outTopicName,windowedSerde.deserializer(), new LongDeserializer());
        outTopic = testDriver.createOutputTopic(outTopicName, new StringDeserializer(), new LongDeserializer());
        errorTopic = testDriver.createOutputTopic(errorTopicName, new StringDeserializer(), new StringDeserializer());
    }

    @AfterAll
    public static void close(){
        testDriver.close();
    }
}
```

- We have the setup for the TestTopology. Now we can add a test that will insert two events into the topic.

```java
    @Test
    public void shouldHaveOneTransaction() {
        // A FinancialMessage is mocked and set to the input topic. Within the Topology,
        // this gets sent to the outTopic because a userId exists for the incoming message.

        FinancialMessage mock = new FinancialMessage(
            "1", "MET", "SWISS", 12, 1822.38, 21868.55, 94, 7, true
        );
        FinancialMessage mock2 = new FinancialMessage(
            "2", "ASDF", "HELLO", 5, 1000.22, 4444.12, 38, 6, true
        );

        inTopic.pipeInput("T01", mock);
        inTopic.pipeInput("T02", mock2);

        Assertions.assertFalse(outTopic.isEmpty());
        Assertions.assertEquals(1, outTopic.readKeyValue().value);

        KeyValueStore<String,ValueAndTimestamp<FinancialMessage>> store = testDriver.getTimestampedKeyValueStore(storeName);
        Assertions.assertEquals(1, store.approximateNumEntries());
    }
```

- The state store (storeSupplier) has two records input. We have a value count when a groupBy is performed. This particular
test will fail of course, due to the fact that we inserted two records but our test expects one. To remedy this test we can change
`Assertions.assertEquals(1, store.approximateNumEntries());` the 1 to a 2.

- Next let's add another very simple test.

```java
    @Test
    public void testErrorTopicIsNotEmpty() {
        FinancialMessage mock = new FinancialMessage(
            null, "MET", "SWISS", 12, 1822.38, 21868.55, 94, 7, true
        );

        inTopic.pipeInput("T03", mock);

        Assertions.assertFalse(errorTopic.isEmpty());
    }
}
```

As you can see here our message payload is created with `null` for the userId field and the purpose of the test
is to check if our `errorTopic` is empty. Since our `errorTopic.isEmpty()` resolves to false and our assertion
is asserting that it is false as well, thus the test passes.


- Now that we have two simple tests, let's update our first branch to allow us to filter the stream on a condition
that we want. Let's edit our `branches[1]` statement so that it will filter out and retain only the records where
the `totalCost` is greater than 5000.

*Note* Since we are changing the logic of how `branches[1]` functions the `shouldHaveOneTransaction` test function will no longer
function as we intended. Comment out the `@Test` annotation or the whole function before proceeding.

```java
branches[1].filter(
            (key, value) -> (value.totalCost > 5000)
        )
        .groupBy(
            (key, value) -> value.userId
        )
        .count(
            Materialized.as(storeSupplier)
        )
        .toStream()
        .to(
            outTopicName,
            Produced.with(Serdes.String(), Serdes.Long())
        );
```

- Now let's create a test that will insert two records and check if we have two records in the output topic.

```java
    @Test
    public void filteredStreamHasTwoRecords() {

        FinancialMessage mock = new FinancialMessage(
            "1", "MET", "SWISS", 12, 1822.38, 21868.55, 94, 7, true
        );
        FinancialMessage mock2 = new FinancialMessage(
            "2", "ASDF", "HELLO", 5, 1000.22, 4444.12, 38, 6, true
        );
        inTopic.pipeInput("T01", mock);
        inTopic.pipeInput("T02", mock2);

        KeyValueStore<String,ValueAndTimestamp<FinancialMessage>> store = testDriver.getTimestampedKeyValueStore(storeName);
        Assertions.assertEquals(2, store.approximateNumEntries());
    }
```

This test fails. Why? The `mock2` message has a `totalCost` of `4444.12` which is less than the filter condition of
`totalCost` being greater than 5000. To make this test pass change that value to something like `5001`.


## Next Steps

- Now that you have finished this initial part of Lab 1 you can optionally proceed to [Lab 2](/use-cases/kafka-streams/lab-2/)
