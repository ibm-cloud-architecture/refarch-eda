---
title: Kafka Mirror Maker 2
description: Kafka Mirror Maker 2
---

This section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0).

## General concepts

As [Mirror maker 2.0](https://strimzi.io/docs/master/#con-configuring-mirror-maker-deployment-configuration-kafka-mirror-maker) is using Kafka Connect framework, we recommend to review our summary of Kafka Connect [here](../kafka-connect/).

The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.

![Kafka Connect](../images/mm-k-connect.png)

In distributed mode, MirrorMaker 2.0 creates the following topics on the target cluster:

* mm2-configs.source.internal: This topic is used to store the connector and task configuration.
* mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect.
* mm2-status.source.internal: This topic is used to store status updates of connectors and tasks.
* source.heartbeats
* source.checkpoints.internal

A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol.  IBM Event Streams is a support, enterprise version of Apache Kafka by IBM.

```properties
clusters=source, target
source.bootstrap.servers=${KAFKA_SOURCE_BROKERS}
target.bootstrap.servers=${KAFKA_TARGET_BROKERS}
target.security.protocol=SASL_SSL
target.ssl.protocol=TLSv1.2
target.ssl.endpoint.identification.algorithm=https
target.sasl.mechanism=PLAIN
target.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="token" password=${KAFKA_TARGET_APIKEY};
# enable and configure individual replication flows
source->target.enabled=true
source->target.topics=products
tasks.max=10
```

* Topics are configured to be replicated or not using a _whitelist_ and _blacklist_ concept
* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.
* Blacklisted topics: by default the following pattern is applied:

```properties
blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
```

We can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.

Internally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.

## Scenarios

The following table summarizes the different scenarios we have tested for data replication using both the Strimzi Operator and IBM Event Streams:

| Environment | Source                 | Target                 | Connect |
|:-----------:|------------------------|------------------------|---------|
| 1           | Local                  | Event Streams on Cloud | Local   |
| 2           | Strimzi on OCP         | Event Streams on Cloud | OCP / ROKS |
| 3           | Event Streams on Cloud | Local                  | Local   |
| 4           | Event Streams on Cloud | Strimzi on OCP         | OCP / ROKS |
| 5           | Event Streams on OCP   | Event Streams on Cloud | OCP / ROKS |

\*The connect column defines where the MirrorMaker 2 connect to.

## Where to go next

You can read more about how to configure the different scenarios above, how they work, how to apply security in data replication, how to monitor Mirror Maker and much more detailed information on data replication [**here**](https://ibm-cloud-architecture.github.io/refarch-eda-data-consistency/)