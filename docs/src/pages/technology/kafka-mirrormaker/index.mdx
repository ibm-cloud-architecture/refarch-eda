---
title: Kafka Mirror Maker 2
description: Kafka Mirror Maker 2
---

This section introduces **Mirror Maker 2.0**, the new replication feature of Kafka 2.4, and how it can be used, along with best practices, for data replication between two Kafka clusters. Mirror Maker 2.0 was defined as part of the Kafka Improvement Process - [KIP 382](https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0).

## General concepts

As [Mirror maker 2.0](https://strimzi.io/docs/master/#con-configuring-mirror-maker-deployment-configuration-kafka-mirror-maker) is using Kafka Connect framework, we recommend to review our summary of Kafka Connect [here](../kafka-connect/).

The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect.

![Kafka Connect](../images/mm-k-connect.png)

MirrorMaker 2 uses the cluster name or identifier as prefix for topic, and uses the concept of source topic and target topic. In distributed mode, MirrorMaker 2.0 creates the following topics on the target cluster:

* mm2-configs.source.internal: This topic is used to store the connector and task configuration.
* mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect.
* mm2-status.source.internal: This topic is used to store status updates of connectors and tasks.
* source.heartbeats
* source.checkpoints.internal

A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol.  IBM Event Streams is a support, enterprise version of Apache Kafka by IBM.

```properties
clusters=source, target
source.bootstrap.servers=${KAFKA_SOURCE_BROKERS}
target.bootstrap.servers=${KAFKA_TARGET_BROKERS}
target.security.protocol=SASL_SSL
target.ssl.protocol=TLSv1.2
target.ssl.endpoint.identification.algorithm=https
target.sasl.mechanism=PLAIN
target.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="token" password=${KAFKA_TARGET_APIKEY};
# enable and configure individual replication flows
source->target.enabled=true
source->target.topics=products
tasks.max=10
```

* Topics are configured to be replicated or not using a _whitelist_ and _blacklist_ concept
* White listed topics are set with the `source->target.topics` attribute of the replication flow and uses [Java regular expression](https://www.vogella.com/tutorials/JavaRegularExpressions/article.html) syntax.
* Blacklisted topics: by default the following pattern is applied:

```properties
blacklist = [follower\.replication\.throttled\.replicas, leader\.replication\.throttled\.replicas, message\.timestamp\.difference\.max\.ms, message\.timestamp\.type, unclean\.leader\.election\.enable, min\.insync\.replicas]
```

We can also define the _blacklist_ with the properties: `topics.blacklist`. Comma-separated lists and Java Regular Expressions are supported.

Internally, `MirrorSourceConnector` and `MirrorCheckpointConnector` will create multiple Kafka tasks (up to the value of `tasks.max` property), and `MirrorHeartbeatConnector` creates an additional task. `MirrorSourceConnector` will have one task per topic-partition combination to replicate, while `MirrorCheckpointConnector` will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the `assign()` API, so there is no consumer group used while fetching data from source topics. There is no call to `commit()` either; rebalancing occurs only when there is a new topic created that matches the _whitelist_ pattern.

## Scenarios

The following table summarizes the different scenarios we have tested for data replication using both the Strimzi Operator and IBM Event Streams:

| Environment | Source                 | Target                 | Connect |
|:-----------:|------------------------|------------------------|---------|
| 1           | Local                  | Event Streams on Cloud | Local   |
| 2           | Strimzi on OCP         | Event Streams on Cloud | OCP / ROKS |
| 3           | Event Streams on Cloud | Local                  | Local   |
| 4           | Event Streams on Cloud | Strimzi on OCP         | OCP / ROKS |
| 5           | Event Streams on OCP   | Event Streams on Cloud | OCP / ROKS |

\*The connect column defines where the MirrorMaker 2 connect to.



To deploy MirrorMaker2 the tool, we can use the Strimzi Kafka latest docker image deployed on Openshift cluster (We address Strimzi deployment in [this note](../../technology/kafka-mirrormaker/)).

To define the clusters and topic configuration we use yaml files. One simple example to replicate from IBM Cloud Event streams to Kafka on premise is in the folder [deployments/strimzi/es-mirror-maker.properties](https://github.com/ibm-cloud-architecture/refarch-eda/blob/master/deployments/strimzi/es-mirror-maker.properties)

Using the same kafka image we can start a mirror maker container with:

```properties
clusters = source, target
source.bootstrap.servers = my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443
source.security.protocol=SSL
source.ssl.truststore.password=password
source.ssl.truststore.location=/home/truststore.jks
target.bootstrap.servers = kafka1:9092
# enable and configure individual replication flows
source->target.enabled = true
source->target.topics = test
```

```bash
./connect-mirror-maker.sh /home/strimzi.properties
```
When Mirror maker starts it will create some topics on source cluster to manage the offsets and topic metadata:

```
mm2-configs.target.internal                                   1            3
mm2-offset-syncs.target.internal                              1            3
mm2-offsets.target.internal                                   25           3
mm2-status.target.internal                                    5            3
```

And on the target cluster:

```
__consumer_offsets
heartbeats
mm2-configs.source.internal
mm2-offsets.source.internal
mm2-status.source.internal
source.checkpoints.internal
source.heartbeats
source.test
```

The `source.test` topic is the replicated `test` topic from the source cluster.

![](../images/mm-k-connect.png)

## Where to go next

You can read more about how to configure the different scenarios above, how they work, how to apply security in data replication, how to monitor Mirror Maker and much more detailed information on data replication [**here**](https://ibm-cloud-architecture.github.io/refarch-eda-data-consistency/)