---
title: Kafka Connect
description: Kafka Connect
---

[Kafka connect](https://kafka.apache.org/documentation/#connect) is an open source component for easily integrate external systems with Kafka. It works with any Kafka producer like IBM Event Streams and Red Hat AMQ Streams.   It uses the concepts of source and sink connectors to ingest or deliver data to / from  Kafka topics.

![Kafka component](images/kafka-components.png)

The general concepts are detailed in the [IBM Event streams product documentation](https://ibm.github.io/event-streams/connecting/connectors/). Here is a quick summary:

* **connector** represents a logical job to move data from / to kafka  to / from external systems. A lot of [existing connectors](https://ibm.github.io/event-streams/connectors/) can be reused, or you can implement your owns.
* **workers** are JVM running the connector. For production deployment workers run in cluster or "distributed mode".
* **tasks**: each worker coordinates a set of tasks to copy data. Task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline.

![Connectors and tasks](images/connector-tasks.png)

When a connector is submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work.

## Characteristics

* Copy vast quantities of data from source to kafka: work at the datasource level. So when it is a database, it uses JDBC API for example.
* Support streaming and batch.
* Scale at the organization level, even if it can support a standalone, mono connector approach to start small, it is possible to run in parallel on distributed cluster.
* Copy data, externalizing transformation in other framework.
* Kafka Connect defines three models: data model, worker model and connector model.
* Provide a REST interface to manage connectors and monitor jobs.

## Installation

The  Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment.

We recommend reading the [IBM  event streams documentation](https://ibm.github.io/event-streams/connecting/setting-up-connectors/) for installing Kafka connect with IBM Event Streams or you can also leverage the [Strimzi Kafka connect operator](https://strimzi.io/docs/0.17.0/#kafka-connect-str).

With IBM Event Streams on premise deployment, the connectors setup is part of the user admin console toolbox:

![Event Streams connector](images/es-connectors.png)

*Deploying connectors against an IBM Event Streams cluster, you need to have API key with permissions to produce and consume messages for all topics.*

As an extendable framework, kafka connect, can have new connector plugins. To deploy new connector, the kafka docker image defining the connector needs to be updated with the connector jar and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL.

Here is the [list of supported connectors](https://ibm.github.io/event-streams/connectors/) for IBM Event Streams.

We will use this image to run the kafka connect in standalone mode or in [the distributed mode section](#distributed-mode).

## Getting started with kafka connect standalone mode

For development and test purposes, we can use Kafka connect in standalone mode, but still connected to IBM Event Streams running on-premise or on-cloud.  

* From the [downloaded dockerfile](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/Dockerfile) (in the refarch-kc repository) we can build a new kafka connect environment image using the command:

```shell
docker build -t ibmcase/kafkaconnect:0.0.1 .
```

* Start a container with kafka connector, to run a standalone connector: you need to use a worker configuration and one of the connector properties file under the `connectors` folder. Those files will be mounted under the `/opt/kafka/config` inside the container. Also, as we want to test sending the content of a file, we mount a local `data` folder to the `/home/data`:

```shell
# in the refarch-kc/docker/kafka-connect folder
docker run -ti  --rm -v $(pwd)/config:/opt/kafka/config -v $(pwd)/data:/home/data --entrypoint bash -p 8083:8083 ibmcase/kafkaconnect:0.0.1
```

!!! Note
        You need to modify those property files to set the API key for your own event streams cluster, and set any other properties.

* Inside the container starts the standalone connector:

```shell
cd /opt/kafka
./bin/connect-standalone.sh config/worker-standalone.properties config/file-source.properties config/file-sink.properties
```

The  `file-source.properties` configures a file reader to source the `data/access_log.txt` file to the `clickstream` topic:

```properties
name=local-file-source
connector.class=FileStreamSource
tasks.max=1
file=/home/kafka-connect/access_log.txt
topic=clickstream
```

While the `config/file-sink.properties` defines a file sink stream to create a json file by getting messages from the `clickstream` topic. The file sink connector can read from multiple topics to aggregate the content in the same file.

The standalone connector worker configuration specifies where to connect, and what converters to use:

```properties
bootstrap.servers=....
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter

# Local storage file for offset data
offset.storage.file.filename=/tmp/connect.offsets
```

The execution trace shows the producer id, and the consumer id, and the task for each connector:

```log
INFO Creating task local-file-source-0 (org.apache.kafka.connect.runtime.Worker:414)
...
INFO TaskConfig values: 
	task.class = class org.apache.kafka.connect.file.FileStreamSourceTask
...
 INFO Creating connector local-file-sink of type FileStreamSink (org.apache.kafka.connect.runtime.Worker:246)
 INFO Creating task local-file-sink-0 (org.apache.kafka.connect.runtime.Worker:414)
```

To validate the data are well published see the generated file under the `data` folder. As the Json converter was used, the message was wrapped into a json document with schema and payload.

```json
{"schema":{"type":"string","optional":false},"payload":"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \"POST /xmlrpc.php HTTP/1.0\" 200 370 \"-\" \"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\""}
```

## Connecting to IBM Cloud Event Streams remote cluster

To connect to Event Streams on IBM Cloud the properties needs to define the broker adviser URLs and the API key that you get from the service crendentials.

This API key must provide permission to produce and consume messages for all topics, and also to create topics.

With Event streams on Cloud the [following document](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect) explains what properties to add to the worker and connectors configuration.

```properties
bootstrap.servers=broker-3-qnsdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprt...
security.protocol=SASL_SSL
ssl.protocol=TLSv1.2
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="token" password="98....";

```

Using the same `file source stream connector` to send records and a simple consumer console to trace the output like:

```shell
docker run -ti  -v $(pwd)/config:/opt/kafka/config --entrypoint bash  ibmcase/kafkaconnect:0.0.1

esuser@3245874dcdd3: cd /opt/kafka/bin/
esuser@3245874dcdd3: ./kafka-console-consumer.sh --bootstrap-server eventstream140-ibm-es-proxy-route-broker-0-eventstreams.apps.green.ocp.csplab.local:443 --consumer.config /opt/kafka/config/console-consumer.properties --topic clickstream --from-beginning"
```

The console-consumer.properties specifies the SASL properties to connect to the remote broker using API key.

## Distributed mode

When running in distributed mode, the connectors need three topics as presented in the `create topics` table [here](https://ibm.github.io/event-streams/connecting/setting-up-connectors/). 

* **connect-configs**: This topic will store the connector and task configurations.
* **connect-offsets**: This topic is used to store offsets for Kafka Connect.
* **connect-status**: This topic will store status updates of connectors and tasks.

* Using IBM Event Streams CLI, the topics are created via the commands like:

```shell
# log to the kubernetes cluster:
cloudctl login -a https://icp-console.apps.green.ocp.csplab.local
# initialize the event streams CLI plugin
cloudctl es init
# Create the Kafka topic
cloudctl es topic-create -n connect-configs -p 1 -r 3 -c cleanup.policy=compact
cloudctl es topic-create -n connect-offsets -p 25 -r 3 -c cleanup.policy=compact
cloudctl es topic-create -n connect-status -p 5 -r 3 -c cleanup.policy=compact
cloudctl es topics
```

* When using a kafka cluster managed with Strimzi topic operator you can use the topic definitions in [the folder](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect):

```shell
oc apply -f strimzi-connect-config-topic.yaml
oc apply -f strimzi-connect-offsets-topic.yaml
oc apply -f strimzi-connect-status-topic.yaml
```

The connector configuration needs to specify some other properties as explained in the [kafka documentation](https://kafka.apache.org/documentation/#connectconfigs)):

* group.id to specify the connect cluster name.
* key and value converters.
* replication factors and topic name for the three needed topics, if Kafka connect is enabled to create topics on the cluster.

When using Event Streams as kafka cluster, add the `sasl` properties as described in the [product documentation](https://cloud.ibm.com/docs/services/EventStreams?topic=eventstreams-kafka_connect#distributed_worker).

With Event Streams as part of the Cloud Pak for integration, the administration console explains the steps to setup connectors, get distributed configuration and how to add connectors.

See [this properties file](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/kafka-connect/distributed-workers.properties) as an example.

To start a Kafka connect in distributed mode locally, connected to Event Streams deployed on-premise use the following command (the entry point in the dockerfile use the connect-distributed mode script):

```shell
docker run -v $(pwd)/config:/opt/kafka/config -p 8083:8083 ibmcase/kafkaconnect:0.0.1
```

To illustrate the Kakfa Connect distributed mode, we will add a source connector from a Mongo DB data source using [this connector](https://www.mongodb.com/kafka-connector).

![Mongo source ](images/kconnect-mongo.png)

When using as a source, the connector publishes data changes from MongoDB into Kafka topics for streaming to consuming apps. Data is captured via Change Streams within the MongoDB cluster and published into Kafka topics. The installation of a connector is done by adding the jars from the connector into the plugin path (`/opt/connectors`) as defined in the connector properties. In the case of mongodb kafka connector the manual installation instructions are in [this github](https://github.com/mongodb/mongo-kafka/blob/master/docs/install.mdx). The download page includes an uber jar.

As we run the kakfa connect as docker container, the approach is to build a new docker image based one of the Kafka image publicly available.

To define and start a connector, you do a POST to the REST API.

## Verifying the connectors via the REST api

The documentation about the REST APIs for the distributed connector is in [this site](https://docs.confluent.io/current/connect/references/restapi.html).

For example the http://localhost:8083/connectors is the base URL when running locally.

## Deploy the Kafka connect as a service within Openshift cluster

When you use IBM Event Streams on Openshift, you can deploy the IBM kafka connector environment as Docker containers, and define the needed `connect-*` topics as explained in previous section. The product documentation describes how to do that.

Another approach is to use [Strimzi](https://strimzi.io/) operator.

To Be done! 

## Running with local kafka cluster

We are using a local kafka cluster started with docker-compose as defined in the compose file [here](https://github.com/ibm-cloud-architecture/refarch-kc/blob/master/docker/backbone-compose.yml).

* The docker network should be `kafkanet`, if not already created do the following

```shell
docker network create kafkanet
```

* Start the kafka broker (bitnami distribution) and zookeeper node using the command below under the `refarch-kc/docker` folder:

```shell
docker-compose -f backbone-compose.yml up -d
```

* Start a container with kafka code, to run a standalone connector: you need to use a worker configuration and a connector properties files. Those files will be mounted under the /home folder:

```shell
docker run -ti  -rm --name kconnect -v $(pwd):/home --network kafkanet -p 8083:8083 bitnami/kafka:2 bash
```

Need to map the port 8083, to access the REST APIs.

* Inside the container starts the standalone connector:

```shell
cd /opt/bitnami/kafka
./bin/connect-standalone.sh /home/kafka-connect/worker-standalone.properties /home/kafka-connect/file-source.properties
```

The above file configures a file reader to source the `access_log.txt` file to the `clickstream` topic:

```properties
name=local-file-source
connector.class=FileStreamSource
tasks.max=1
file=/home/kafka-connect/access_log.txt
topic=clickstream
```

The standalone connector worker configuration specifies where to connect, and what converters to use:

```properties
bootstrap.servers=kafka1:9092
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter

# Local storage file for offset data
offset.storage.file.filename=/tmp/connect.offsets
```

The execution trace shows the producer id

```log
INFO [Producer clientId=connector-producer-local-file-source-0] Cluster ID: tj8y0hiZSYWHB9vLHGP1Ew (org.apache.kafka.clients.Metadata:261)
```

To validate the data are well published run another container with the consumer console tool:

```shell
docker run -ti  --name sinktrace --rm  --network kafkanet bitnami/kafka:2 bash -c "
/opt/bitnami/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic clickstream --from-beginning"
```

As the Json converter was used the trace show the message was wrapped into a json document with schema and payload.

```json
{"schema":{"type":"string","optional":false},"payload":"46.166.139.20 - - [01/Dec/2015:23:22:09 +0000] \"POST /xmlrpc.php HTTP/1.0\" 200 370 \"-\" \"Mozilla/4.0 (compatible: MSIE 7.0; Windows NT 6.0)\""}
```

## Further Readings

* [Apache Kafka connect documentation](https://kafka.apache.org/documentation/#connect)
* [Confluent Connector Documentation](https://docs.confluent.io/current/connect/index.html)
* [IBM Event Streams Connectors](https://ibm.github.io/event-streams/connecting/connectors/) or [the list of supported connectors](https://ibm.github.io/event-streams/connectors/)
* [MongoDB Connector for Apache Kafka](https://github.com/mongodb/mongo-kafka)
